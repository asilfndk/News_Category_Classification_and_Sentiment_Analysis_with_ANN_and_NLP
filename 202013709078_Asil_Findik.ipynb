{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**News Category Classification**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install feedparser\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def veri_yukle():\n",
    "    veri = pd.read_excel('news_category.xlsx')\n",
    "    metinler = veri['HABERLER'].values\n",
    "    etiketler = veri['ETIKET'].values\n",
    "    return veri, metinler, etiketler\n",
    "\n",
    "def veri_tokenize(veri):\n",
    "    veri['Cumle'] = veri['HABERLER'].apply(lambda x: sent_tokenize(str(x)))\n",
    "    veri['Kelime'] = veri['HABERLER'].apply(lambda x: word_tokenize(str(x)))\n",
    "    return veri\n",
    "\n",
    "def stopwords_filtrele(veri):\n",
    "    stopwords_listesi = stopwords.words(\"turkish\")\n",
    "    filtrelenmis_kelimeler = [\n",
    "        [kelime for kelime in kelime_listesi if kelime.lower() not in stopwords_listesi]\n",
    "        for kelime_listesi in veri['Kelime']\n",
    "    ]\n",
    "    return filtrelenmis_kelimeler\n",
    "\n",
    "def word2vec_egit(filtrelenmis_kelimeler):\n",
    "    word2vec_modeli = Word2Vec(sentences=filtrelenmis_kelimeler, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    with open('word2vec_model.py', 'wb') as dosya:\n",
    "        pickle.dump(word2vec_modeli, dosya)\n",
    "    return word2vec_modeli\n",
    "\n",
    "def veri_onisleme(X, y):\n",
    "    etiket_encoder = LabelEncoder()\n",
    "    y = etiket_encoder.fit_transform(y)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_matrix(X, mode='binary')\n",
    "    return X, y\n",
    "\n",
    "def model_olustur_egit(X_egitim, y_egitim, X_test, y_test):\n",
    "    model = Sequential([\n",
    "      Dense(256, input_shape=(X_egitim.shape[1],), activation='relu'),\n",
    "      Dense(128, activation='relu'),\n",
    "      Dropout(0.3),\n",
    "      Dense(64, activation='relu'),\n",
    "      Dense(32, activation='relu'),\n",
    "      Dropout(0.2),\n",
    "      Dense(len(set(y_egitim)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    modelsekil = model.fit(X_egitim, y_egitim, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n",
    "    return model\n",
    "\n",
    "def modeli_degerlendir(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "def main():\n",
    "    veri, X, y = veri_yukle()\n",
    "    veri = veri_tokenize(veri)\n",
    "    filtrelenmis_kelimeler = stopwords_filtrele(veri)\n",
    "    word2vec_model = word2vec_egit(filtrelenmis_kelimeler)\n",
    "    X, y = veri_onisleme(X, y)\n",
    "    X_egitim, X_test, y_egitim, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = model_olustur_egit(X_egitim, y_egitim, X_test, y_test)\n",
    "    modeli_degerlendir(model, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "feed_url = \"https://www.cnnturk.com/feed/rss/all/news\"\n",
    "parsed_feed = feedparser.parse(feed_url)\n",
    "haberler = [entry.title for entry in parsed_feed.entries]\n",
    "haberler_encoded = tokenizer.texts_to_matrix(haberler, mode='binary')\n",
    "\n",
    "olasiliklar = model.predict(haberler_encoded)\n",
    "etiket_etiketleri = np.argmax(olasiliklar, axis=1)\n",
    "etiket = etiket_encoder.inverse_transform(etiket_etiketleri)\n",
    "\n",
    "df = pd.DataFrame({'HABERLER': haberler, 'ETIKET': etiket})\n",
    "df.to_csv('HABERLER.csv', index=False)\n",
    "\n",
    "from google.colab import files\n",
    "files.download('HABERLER.csv')\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "#Ağ Şekli\n",
    "model = Sequential([\n",
    "    Dense(256, input_shape=(X_egitim.shape[1],), activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(len(set(y_egitim)), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Eğitim sırasında kayıp ve doğruluk metriklerini görselleştirme\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Eğitim ve doğrulama kaybı\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Eğitim Kaybı')\n",
    "plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')\n",
    "plt.title('Eğitim ve Doğrulama Kaybı')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Kayıp')\n",
    "plt.legend()\n",
    "\n",
    "# Eğitim ve doğrulama doğruluğu\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')\n",
    "plt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')\n",
    "plt.title('Eğitim ve Doğrulama Doğruluğu')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Doğruluk')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Test verileri üzerinde modeli değerlendirme\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Hata matrisi oluşturma\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Hata matrisini görselleştirme\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues')\n",
    "plt.ylabel('Gerçek Etiket')\n",
    "plt.xlabel('Öngörülen Etiket')\n",
    "all_sample_title = 'Sınıflandırma Raporu'\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Değerlendirme Metriklerinin Hesaplanması\n",
    "y_tahmin = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Hassasiyet\n",
    "hassasiyet = precision_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'Hassasiyet: {hassasiyet}')\n",
    "\n",
    "# Duyarlılık\n",
    "duyarlılık = recall_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'Duyarlılık: {duyarlılık}')\n",
    "\n",
    "# F1 Skoru\n",
    "f1 = f1_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'F1 Skoru: {f1}')\n",
    "\n",
    "# Eğitilmiş modeli kullanarak olasılıkları hesaplama\n",
    "olasiliklar = model.predict(X_test)\n",
    "\n",
    "# Çok sınıflı ROC eğrileri için etiketleri tek seferde kodlama\n",
    "y_test_one_hot = label_binarize(y_test, classes=np.unique(y_test))\n",
    "\n",
    "# Her sınıf için ROC eğrisini ve ROC alanını hesaplama\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], olasiliklar[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Her sınıf için ROC eğrilerini çizdirme\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'ROC Eğrisi (Sınıf {i}) (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red')\n",
    "plt.xlabel('Yanlış Pozitif Oranı')\n",
    "plt.ylabel('Doğru Pozitif Oranı')\n",
    "plt.title('ROC Eğrileri - Çoklu Sınıf')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Sentiment Analysis**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install feedparser\n",
    "import feedparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def veri_yukle():\n",
    "    veri = pd.read_csv('sentiment_analysis.csv')\n",
    "    X = veri['text'].values\n",
    "    y = veri['label'].values\n",
    "    return veri, X, y\n",
    "\n",
    "def cumleleri_tokenlestir(veri):\n",
    "    veri['CUMLE'] = veri['text'].apply(lambda x: sent_tokenize(str(x)))\n",
    "    veri['KELIME'] = veri['text'].apply(lambda x: word_tokenize(str(x)))\n",
    "    return veri\n",
    "\n",
    "def stopwords_filtrele(veri):\n",
    "    stopwords_listesi = set(stopwords.words(\"turkish\"))\n",
    "    veri['FILTRELI_KELIME'] = veri['KELIME'].apply(lambda x: [kelime for kelime in x if kelime.lower() not in stopwords_listesi])\n",
    "    return veri\n",
    "\n",
    "def word2vec_egit(filtered_words):\n",
    "    word2vec_modeli = Word2Vec(sentences=filtered_words, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    with open('word2vec_model.py', 'wb') as dosya:\n",
    "        pickle.dump(word2vec_modeli, dosya)\n",
    "    return word2vec_modeli\n",
    "\n",
    "def veriyi_onisle(X, y):\n",
    "    etiket_encoder = LabelEncoder()\n",
    "    y = etiket_encoder.fit_transform(y)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    X = tokenizer.texts_to_matrix(X, mode='binary')\n",
    "    return X, y\n",
    "\n",
    "def model_olustur(X_egitim, y_egitim, X_test, y_test):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_egitim.shape[1],), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(len(set(y_egitim)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_egitim, y_egitim, epochs=15, batch_size=32, validation_data=(X_test, y_test))\n",
    "    return model\n",
    "\n",
    "def modeli_degerlendir(model, X_test, y_test):\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "def main():\n",
    "    data, X, y = veri_yukle()\n",
    "    data = cumleleri_tokenlestir(data)\n",
    "    filtered_words = stopwords_filtrele(data)\n",
    "    word2vec_model = word2vec_egit(filtered_words)\n",
    "    X, y = veriyi_onisle(X, y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = model_olustur(X_train, y_train, X_test, y_test)\n",
    "    modeli_degerlendir(model, X_test, y_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "url = 'https://www.haberler.com/ekonomi/turk-is-ten-hukumete-asgari-ucret-resti-aclik-16605261-haberi/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "yorum_etiketleri = soup.find_all(['div'], class_=['hbcRow', 'hbcInRow'])\n",
    "\n",
    "yorum_listesi = []\n",
    "for yorum_etiketi in yorum_etiketleri:\n",
    "    yorum_listesi.append(yorum_etiketi.text.strip())\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(yorum_listesi)\n",
    "yorumlar_encoded = tokenizer.texts_to_matrix(yorum_listesi, mode='binary')\n",
    "\n",
    "kelime_vektorleri_yorum = []\n",
    "for yorum in yorum_listesi:\n",
    "    kelimeler = word_tokenize(yorum)\n",
    "    vektor = np.zeros(100)\n",
    "    sayac = 0\n",
    "    for kelime in kelimeler:\n",
    "        if kelime in word2vec_modeli.wv:\n",
    "            vektor += word2vec_modeli.wv[kelime]\n",
    "            sayac += 1\n",
    "    if sayac != 0:\n",
    "        vektor /= sayac\n",
    "    kelime_vektorleri_yorum.append(vektor)\n",
    "\n",
    "padded_sequences = pad_sequences(kelime_vektorleri_yorum, maxlen=100, padding='post', truncating='post')\n",
    "yorumlar_encoded = np.array(padded_sequences)\n",
    "\n",
    "tahmin_edilen_olasiliklar = model.predict(yorumlar_encoded)\n",
    "tahmin_edilen_etiketler = np.argmax(tahmin_edilen_olasiliklar, axis=1)\n",
    "tahmin_edilen_kategoriler = etiket_encoder.inverse_transform(tahmin_edilen_etiketler)\n",
    "\n",
    "\n",
    "tahmin_edilen_veri = pd.DataFrame({'text': yorum_listesi, 'label': tahmin_edilen_kategoriler})\n",
    "tahmin_edilen_veri.to_csv('Tahmin_Edilen_Yorumlar.csv', index=False, encoding='utf-8')\n",
    "\n",
    "#Ağ Şekli\n",
    "model = Sequential([\n",
    "        Dense(64, input_shape=(X_egitim.shape[1],), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(len(set(y_egitim)), activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Eğitim sırasında kayıp ve doğruluk metriklerini görselleştirme\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Eğitim ve doğrulama kaybı\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Eğitim Kaybı')\n",
    "plt.plot(history.history['val_loss'], label='Doğrulama Kaybı')\n",
    "plt.title('Eğitim ve Doğrulama Kaybı')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Kayıp')\n",
    "plt.legend()\n",
    "\n",
    "# Eğitim ve doğrulama doğruluğu\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Eğitim Doğruluğu')\n",
    "plt.plot(history.history['val_accuracy'], label='Doğrulama Doğruluğu')\n",
    "plt.title('Eğitim ve Doğrulama Doğruluğu')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Doğruluk')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Test verileri üzerinde modeli değerlendirme\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "# Hata matrisi oluşturma\n",
    "cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "# Hata matrisini görselleştirme\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues')\n",
    "plt.ylabel('Gerçek Etiket')\n",
    "plt.xlabel('Öngörülen Etiket')\n",
    "all_sample_title = 'Sınıflandırma Raporu'\n",
    "plt.title(all_sample_title, size = 15)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Değerlendirme Metriklerinin Hesaplanması\n",
    "y_tahmin = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Hassasiyet\n",
    "hassasiyet = precision_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'Hassasiyet: {hassasiyet}')\n",
    "\n",
    "# Duyarlılık\n",
    "duyarlılık = recall_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'Duyarlılık: {duyarlılık}')\n",
    "\n",
    "# F1 Skoru\n",
    "f1 = f1_score(y_test, y_tahmin, average='weighted')\n",
    "print(f'F1 Skoru: {f1}')\n",
    "\n",
    "# Eğitilmiş modeli kullanarak olasılıkları hesaplama\n",
    "olasiliklar = model.predict(X_test)\n",
    "\n",
    "# Çok sınıflı ROC eğrileri için etiketleri tek seferde kodlama\n",
    "y_test_one_hot = label_binarize(y_test, classes=np.unique(y_test))\n",
    "\n",
    "# Her sınıf için ROC eğrisini ve ROC alanını hesaplama\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], olasiliklar[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Her sınıf için ROC eğrilerini çizdirme\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(len(np.unique(y_test))):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'ROC Eğrisi (Sınıf {i}) (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='red')\n",
    "plt.xlabel('Yanlış Pozitif Oranı')\n",
    "plt.ylabel('Doğru Pozitif Oranı')\n",
    "plt.title('ROC Eğrileri - Çoklu Sınıf')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
